# WebAI

Небольшое FastAPI-приложение для общения с локальной Ollama-моделью через LangChain.

## Конфигурация
Переменные окружения читаются через `pydantic-settings` (см. `config.py`):

| Переменная            | Значение по умолчанию           | Описание                                      |
|-----------------------|---------------------------------|-----------------------------------------------|
| `HOST`                | `127.0.0.1`                     | Хост для запуска API                          |
| `PORT`                | `8000`                          | Порт API                                      |
| `WORKERS`             | `1`                             | Количество воркеров uvicorn                   |
| `OLLAMA_MODEL`        | `gemma3:1b`                     | Имя модели Ollama                             |
| `OLLAMA_BASE_URL`     | `http://127.0.0.1:11434`        | Базовый URL сервиса Ollama                    |

Если приложение работает в Docker, а Ollama запущена на хосте, укажите доступный URL
в `OLLAMA_BASE_URL` (например, `http://host.docker.internal:11434`). Без этого
запросы к генерации будут завершаться ошибкой `503` из-за недоступности сервиса.
